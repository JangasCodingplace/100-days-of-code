{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1 - (NLP) Tokenization\n",
    "\n",
    "Tokenization is the process of splitting a string of text into individual words or punctuation marks called tokens. In natural language processing (NLP), tokenization is a crucial step because it allows the model to work with the text in a more structured and manageable way. For example, tokenization can help identify the words in a sentence, which can then be used to determine the part of speech for each word or to look up the word in a dictionary. Tokenization is often used as a preprocessing step in many NLP tasks, such as language modeling, text classification, and sentiment analysis.\n",
    "\n",
    "Suppose we have the following sentence: `The quick brown fox jumped over the lazy dog.`\n",
    "\n",
    "We can split this sentence into individual tokens (words and punctuation marks) as follows:\n",
    "\n",
    "```\n",
    "[\"The\", \"quick\", \"brown\", \"fox\", \"jumped\", \"over\", \"the\", \"lazy\", \"dog\", \".\"]\n",
    "```\n",
    "\n",
    "Each token in the list above is an individual word or punctuation mark from the original sentence. This list of tokens can then be used for various NLP tasks, such as identifying the part of speech for each word or looking up the words in a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Punctuation** refers to the symbols that are used in writing to separate sentences and clauses, and to indicate the structure and organization of a text. Some examples of punctuation marks include the period (.), the comma (,), the exclamation point (!), and the question mark (?). Punctuation marks are often included in the list of tokens when a string of text is tokenized, which means they are treated as individual units in the same way that words are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple example of a tokenization script in pure Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize a string of text into a list of tokens.\n",
    "    \n",
    "    This function uses a regular expression to split the input text into a list of tokens. Any sequence of non-word characters\n",
    "    (i.e., anything that is not a letter, digit, or underscore) is used as a delimiter to separate the tokens.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The input string of text to be tokenized.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tokens : list of str\n",
    "        A list of tokens, where each token is an individual word or punctuation mark from the input text.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> tokenize(\"The quick brown fox jumped over the lazy dog.\")\n",
    "    [\"The\", \"quick\", \"brown\", \"fox\", \"jumped\", \"over\", \"the\", \"lazy\", \"dog\", \".\"]\n",
    "    \"\"\"\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens\n",
    "\n",
    "text = \"The quick brown fox jumped over the lazy dog.\"\n",
    "\n",
    "print(tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note that this example does not include the punctuation marks in the list of tokens, because the regular expression used to split the text matches any sequence of non-word characters, including punctuation marks. To include the punctuation marks in the list of tokens, you would need to modify the regular expression to match only individual punctuation marks, rather than sequences of them._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of tokenization using NLTK (Natural Language Toolkit) in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"The quick brown fox jumped over the lazy dog.\"\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "Unit tests are small, isolated tests that check the behavior of individual units of code. They help you catch bugs and other problems in your code early on, and provide a form of documentation for your code. By running a suite of unit tests, you can ensure the quality and reliability of your code, and save time and effort by catching and fixing problems early on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the tokenize function with a simple sentence\n",
    "def test_tokenize_simple():\n",
    "    text = \"The quick brown fox jumped over the lazy dog.\"\n",
    "    expected = [\"The\", \"quick\", \"brown\", \"fox\", \"jumped\", \"over\", \"the\", \"lazy\", \"dog\", \"\"]\n",
    "    actual = tokenize(text)\n",
    "    assert actual == expected\n",
    "\n",
    "# Test the tokenize function with a sentence that contains multiple punctuation marks\n",
    "def test_tokenize_punctuation():\n",
    "    text = \"Hello, world! How are you today?\"\n",
    "    expected = [\"Hello\", \"world\", \"How\", \"are\", \"you\", \"today\", \"\"]\n",
    "    actual = tokenize(text)\n",
    "    assert actual == expected\n",
    "\n",
    "# Test the tokenize function with a sentence that contains contractions\n",
    "def test_tokenize_contractions():\n",
    "    text = \"I'm going to the store. Can't you come with me?\"\n",
    "    expected = ['I', 'm', 'going', 'to', 'the', 'store', 'Can', 't', 'you', 'come', 'with', 'me', '']\n",
    "    actual = tokenize(text)\n",
    "    assert actual == expected\n",
    "\n",
    "# Test the tokenize function with an empty string\n",
    "def test_tokenize_empty():\n",
    "    text = \"\"\n",
    "    expected = ['']\n",
    "    actual = tokenize(text)\n",
    "    assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenize_simple()\n",
    "test_tokenize_punctuation()\n",
    "test_tokenize_contractions()\n",
    "test_tokenize_empty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
